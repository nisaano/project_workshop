import re
import string
from collections import Counter
from typing import List, Set, Tuple
import heapq

class AdvancedTextEnhancer:
    def __init__(self):
        # –£–º–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ —Å –≤–µ—Å–∞–º–∏
        self.stop_words = self._load_stop_words()
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –≤–∞–∂–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        self.patterns = {
            'dates': [
                r'\b\d{1,2}\.\d{1,2}\.\d{4}\b',
                r'\b\d{4}\.\d{1,2}\.\d{1,2}\b',
                r'\b\d{1,2}/\d{1,2}/\d{4}\b',
                r'\b\d{4}-\d{1,2}-\d{1,2}\b',
                r'\b\d{4}\b'
            ],
            'acronyms': r'\b[A-Z–ê-–Ø]{2,6}\b',
            'numbers': r'\b\d+[.,]?\d*\b',
            'capitalized': r'\b[–ê-–ØA-Z][–∞-—èa-z]{3,}\b'
        }
        
        # –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (—Å–ª–æ–≤–∞, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ –≤–∞–∂–Ω–æ—Å—Ç—å)
        self.importance_indicators = {
            '–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ': 3, '–ø–æ–Ω—è—Ç–∏–µ': 2, '—Ç–µ—Ä–º–∏–Ω': 3, '–∫–æ–Ω—Ü–µ–ø—Ü–∏—è': 3,
            '—Ç–µ–æ—Ä–∏—è': 2, '–º–µ—Ç–æ–¥': 2, '–∞–ª–≥–æ—Ä–∏—Ç–º': 3, '—Ñ–æ—Ä–º—É–ª–∞': 3,
            '–∑–∞–∫–æ–Ω': 3, '–ø—Ä–∏–Ω—Ü–∏–ø': 2, '—Å–≤–æ–π—Å—Ç–≤–æ': 2, '—Ñ—É–Ω–∫—Ü–∏—è': 2,
            '—Å—Ç—Ä—É–∫—Ç—É—Ä–∞': 2, '–ø—Ä–æ—Ü–µ—Å—Å': 2, '—Å–∏—Å—Ç–µ–º–∞': 2, '–º–æ–¥–µ–ª—å': 2,
            '–≤–∞–∂–Ω–æ': 1, '–∫–ª—é—á–µ–≤–æ–π': 2, '–æ—Å–Ω–æ–≤–Ω–æ–π': 2, '–≥–ª–∞–≤–Ω—ã–π': 2
        }

    def _load_stop_words(self) -> Set[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤"""
        base_stop_words = {
            # –†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            '–∏', '–≤', '–≤–æ', '–Ω–∞', '—Å', '–ø–æ', '–∫', '—É', '–æ', '–∏–∑', '–∑–∞', '–æ—Ç', '–¥–æ',
            '–Ω–µ', '—á—Ç–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '—Ç–∞–∫', '—ç—Ç–æ', '–Ω–æ', '–æ–Ω–∏', '–º—ã',
            '–≤—ã', '–µ–≥–æ', '–µ–µ', '–∏—Ö', '—ç—Ç–æ—Ç', '—Ç–æ—Ç', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä—ã–µ', '—ç—Ç–æ–º',
            '–≤–æ—Ç', '–∏–ª–∏', '–µ—Å–ª–∏', '–ø—Ä–∏', '—Ç–∞–∫–∂–µ', '–¥–ª—è', '—Å–æ', '—Ç–æ', '–∂–µ', '–±—ã',
            '–ª–∏', '–ø–æ', '–¥–æ', '–Ω–µ—Ç', '–¥–∞', '–Ω—É', '–≤—ã', '–º–Ω–µ', '–º–µ–Ω—è', '—Ç–µ–±–µ', '—Ç–µ–±—è',
            '–µ–º—É', '–µ–π', '–Ω–∞–º', '–≤–∞–º', '–∏–º–∏', '–Ω–∏–º–∏', '–æ–ø–∏—Å—ã–≤–∞–µ—Ç', '—è–≤–ª—è–µ—Ç—Å—è',
            '–≥–æ–≤–æ—Ä–∏—Ç', '–±—ã–ª', '–±—ã–ª–∞', '–∏–º–µ–µ—Ç', '–º–æ–≥—É—Ç', '–º–æ–∂–µ—Ç', '—ç—Ç–æ–º', '–∫–∞–∫–æ–π',
            '–∫–æ–≥–¥–∞', '–≥–¥–µ', '—á–µ–º', '–ø–æ—á–µ–º—É', '—Ö–æ—Ç—è', '–ø–æ—Å–ª–µ', '–ø–µ—Ä–µ–¥', '–º–µ–∂–¥—É',
            
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'as', 'is', 'are', 'was', 'were', 'be', 'been',
            'this', 'that', 'these', 'those', 'have', 'has', 'had', 'do', 'does',
            'did', 'will', 'would', 'could', 'should', 'can', 'may', 'might'
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è –∏ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –≥–ª–∞–≥–æ–ª—ã
        pronouns = {'—è', '—Ç—ã', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–æ', '–º—ã', '–≤—ã', '–æ–Ω–∏', '—Å–µ–±—è'}
        verbs = {'–µ—Å—Ç—å', '–±—ã—Ç—å', '—Å—Ç–∞—Ç—å', '—è–≤–ª—è—Ç—å—Å—è', '–Ω–∞–∑—ã–≤–∞—Ç—å—Å—è', '—Å—á–∏—Ç–∞—Ç—å—Å—è'}
        
        return base_stop_words | pronouns | verbs

    def _calculate_sentence_importance(self, sentence: str, key_terms: Set[str]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        words = self._tokenize_text(sentence.lower())
        
        importance_score = 0
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        term_count = sum(1 for word in words if word in key_terms)
        importance_score += term_count * 2
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –≤–∞–∂–Ω–æ—Å—Ç–∏
        for indicator, weight in self.importance_indicators.items():
            if indicator in sentence.lower():
                importance_score += weight
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º –¥–ª–∏–Ω—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—Å—Ä–µ–¥–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ–±—ã—á–Ω–æ –≤–∞–∂–Ω–µ–µ)
        word_count = len(words)
        if 8 <= word_count <= 25:
            importance_score += 1
        
        return importance_score

    def _tokenize_text(self, text: str) -> List[str]:
        """–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ñ–∏—Å—ã –≤ —Å–ª–æ–≤–∞—Ö
        text = re.sub(r'[^\w\s-]', ' ', text)
        words = text.lower().split()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
        return [word for word in words if word not in self.stop_words and len(word) > 2]

    def remove_repetitive_phrases(self, text: str) -> str:
        """–£–º–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ñ—Ä–∞–∑ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–º—ã—Å–ª–∞"""
        sentences = self._split_into_sentences(text)
        
        if len(sentences) <= 1:
            return text
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏
        all_words = []
        for sentence in sentences:
            all_words.extend(self._tokenize_text(sentence))
        
        word_freq = Counter(all_words)
        key_terms = {word for word, count in word_freq.most_common(10) if count >= 2}
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        scored_sentences = []
        for i, sentence in enumerate(sentences):
            score = self._calculate_sentence_importance(sentence, key_terms)
            scored_sentences.append((score, i, sentence))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –∏ –±–µ—Ä–µ–º —Ç–æ–ø-80% –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        scored_sentences.sort(reverse=True)
        keep_count = max(3, int(len(sentences) * 0.8))  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∏–Ω–∏–º—É–º 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        kept_sentences = [sentence for _, _, sentence in scored_sentences[:keep_count]]
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Ä—è–¥–æ–∫
        kept_indices = sorted([i for _, i, _ in scored_sentences[:keep_count]])
        final_sentences = [sentences[i] for i in kept_indices]
        
        return ' '.join(final_sentences)

    def _split_into_sentences(self, text: str) -> List[str]:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ —Ç–æ—á–∫–∞–º, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º –∑–Ω–∞–∫–∞–º
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        return sentences

    def improve_paragraph_structure(self, text: str) -> str:
        """–£–ª—É—á—à–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–µ–∫—Å—Ç–∞ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π"""
        sentences = self._split_into_sentences(text)
        
        if len(sentences) <= 2:
            return text
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —Ç–µ–º–∞–º (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
        paragraphs = []
        current_paragraph = []
        
        for i, sentence in enumerate(sentences):
            current_paragraph.append(sentence)
            
            # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ –µ—Å–ª–∏:
            # 1. –î–æ—Å—Ç–∏–≥–ª–∏ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ò —Å–ª–µ–¥—É—é—â–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –Ω–æ–≤–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            # 2. –í –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –µ—Å—Ç—å –º–∞—Ä–∫–µ—Ä—ã –Ω–∞—á–∞–ª–∞ –Ω–æ–≤–æ–π —Ç–µ–º—ã
            should_break = (
                (len(current_paragraph) >= 2 and i < len(sentences) - 1 and
                 self._is_new_topic(sentences[i], sentences[i+1])) or
                len(current_paragraph) >= 3 or
                any(marker in sentence.lower() for marker in ['—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º', '–≤ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ', '–∫—Ä–æ–º–µ —Ç–æ–≥–æ'])
            )
            
            if should_break and current_paragraph:
                paragraphs.append(' '.join(current_paragraph))
                current_paragraph = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        if current_paragraph:
            paragraphs.append(' '.join(current_paragraph))
        
        return '\n\n'.join(paragraphs)

    def _is_new_topic(self, current_sentence: str, next_sentence: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ –Ω–æ–≤–∞—è —Ç–µ–º–∞"""
        new_topic_indicators = [
            '—Ç–∞–∫–∂–µ', '–∫—Ä–æ–º–µ', '–æ–¥–Ω–∞–∫–æ', '–ø–æ—ç—Ç–æ–º—É', '—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ',
            '–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ', '–≤ –æ—Ç–ª–∏—á–∏–µ', '–Ω–∞–ø—Ä–∏–º–µ—Ä', '–≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏'
        ]
        
        next_lower = next_sentence.lower()
        return any(indicator in next_lower for indicator in new_topic_indicators)

    def extract_key_terms(self, text: str, top_n: int = 8) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        words = self._tokenize_text(text)
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º —Å–æ—Å—Ç–∞–≤–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã (2-3 —Å–ª–æ–≤–∞)
        bigrams = [f"{words[i]} {words[i+1]}" for i in range(len(words)-1)]
        trigrams = [f"{words[i]} {words[i+1]} {words[i+2]}" for i in range(len(words)-2)]
        
        all_terms = words + bigrams + trigrams
        
        # –í–∑–≤–µ—à–∏–≤–∞–µ–º —Ç–µ—Ä–º–∏–Ω—ã
        term_weights = {}
        for term in all_terms:
            term_weights[term] = term_weights.get(term, 0) + 1
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏ —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å –∑–∞–≥–ª–∞–≤–Ω—ã–º–∏ –±—É–∫–≤–∞–º–∏
            if any(word.istitle() for word in term.split()):
                term_weights[term] += 2
            if len(term) > 10:
                term_weights[term] += 1
        
        # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-N —Ç–µ—Ä–º–∏–Ω–æ–≤
        top_terms = heapq.nlargest(top_n * 2, term_weights.items(), key=lambda x: x[1])
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –æ–±—â–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        filtered_terms = []
        for term, score in top_terms:
            if (score >= 2 and 
                len(term) >= 4 and 
                not self._is_too_general(term)):
                filtered_terms.append(term)
        
        return filtered_terms[:top_n]

    def _is_too_general(self, term: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ—Ä–º–∏–Ω —Å–ª–∏—à–∫–æ–º –æ–±—â–∏–º"""
        general_terms = {
            '–º–æ–∂–µ—Ç –±—ã—Ç—å', '—Ç–∞–∫–∂–µ –∫–∞–∫', '–æ–¥–Ω–∞–∫–æ —ç—Ç–æ', '–∫—Ä–æ–º–µ —Ç–æ–≥–æ',
            '–≤ —Ç–æ–º —á–∏—Å–ª–µ', '–ø–æ—ç—Ç–æ–º—É –º–æ–∂–Ω–æ', '—Å–ª–µ–¥—É–µ—Ç –æ—Ç–º–µ—Ç–∏—Ç—å'
        }
        return term in general_terms

    def highlight_key_elements(self, text: str) -> str:
        """–í—ã–¥–µ–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ —Ç–µ–∫—Å—Ç–µ"""
        enhanced_text = text
        
        # 1. –í—ã–¥–µ–ª—è–µ–º –¥–∞—Ç—ã
        for pattern in self.patterns['dates']:
            dates = re.findall(pattern, enhanced_text)
            for date in set(dates):
                enhanced_text = enhanced_text.replace(date, f'**{date}**')
        
        # 2. –í—ã–¥–µ–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        key_terms = self.extract_key_terms(enhanced_text)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ (—Å–Ω–∞—á–∞–ª–∞ –¥–ª–∏–Ω–Ω—ã–µ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤)
        key_terms.sort(key=len, reverse=True)
        
        for term in key_terms:
            # –ò–∑–±–µ–≥–∞–µ–º –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –≤—ã–¥–µ–ª–µ–Ω–∏—è
            if f'**{term}**' not in enhanced_text:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥—Ä–∞–Ω–∏—Ü—ã —Å–ª–æ–≤ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                pattern = r'\b' + re.escape(term) + r'\b'
                enhanced_text = re.sub(pattern, f'**{term}**', enhanced_text, flags=re.IGNORECASE)
        
        # 3. –í—ã–¥–µ–ª—è–µ–º –∞–∫—Ä–æ–Ω–∏–º—ã –∏ –≤–∞–∂–Ω—ã–µ capitalized —Å–ª–æ–≤–∞
        acronyms = re.findall(self.patterns['acronyms'], enhanced_text)
        for acronym in set(acronyms):
            if f'**{acronym}**' not in enhanced_text:
                enhanced_text = enhanced_text.replace(acronym, f'**{acronym}**')
        
        return enhanced_text

    def process_text(self, text: str) -> str:
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
        if not text or len(text.strip()) < 50:
            return text
        
        print("üîß –ù–∞—á–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞...")
        
        # 1. –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ñ—Ä–∞–∑—ã (—É–º–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ)
        text_no_repeats = self.remove_repetitive_phrases(text)
        print("‚úì –£–¥–∞–ª–µ–Ω—ã –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∏ –º–∞–ª–æ–≤–∞–∂–Ω—ã–µ —Ñ—Ä–∞–∑—ã")
        
        # 2. –£–ª—É—á—à–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞–±–∑–∞—Ü–µ–≤
        structured_text = self.improve_paragraph_structure(text_no_repeats)
        print("‚úì –£–ª—É—á—à–µ–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–µ–∫—Å—Ç–∞")
        
        # 3. –í—ã–¥–µ–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        final_text = self.highlight_key_elements(structured_text)
        print("‚úì –í—ã–¥–µ–ª–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ —ç–ª–µ–º–µ–Ω—Ç—ã")
        
        return final_text

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def enhance_text(text: str) -> str:
    enhancer = AdvancedTextEnhancer()
    return enhancer.process_text(text)

# –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç—ã
if __name__ == "__main__":
    sample_text = """
    –ö–≤–∞–Ω—Ç–æ–≤–∞—è –º–µ—Ö–∞–Ω–∏–∫–∞ –æ–ø–∏—Å—ã–≤–∞–µ—Ç –ø–æ–≤–µ–¥–µ–Ω–∏–µ —á–∞—Å—Ç–∏—Ü –Ω–∞ –∞—Ç–æ–º–Ω–æ–º –∏ —Å—É–±–∞—Ç–æ–º–Ω–æ–º —É—Ä–æ–≤–Ω—è—Ö. 
    –ö–≤–∞–Ω—Ç–æ–≤–∞—è –º–µ—Ö–∞–Ω–∏–∫–∞ —è–≤–ª—è–µ—Ç—Å—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π —Ç–µ–æ—Ä–∏–µ–π –≤ —Ñ–∏–∑–∏–∫–µ. –í–æ–ª–Ω–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è 
    —è–≤–ª—è–µ—Ç—Å—è –∫–ª—é—á–µ–≤—ã–º –ø–æ–Ω—è—Ç–∏–µ–º –≤ –∫–≤–∞–Ω—Ç–æ–≤–æ–π –º–µ—Ö–∞–Ω–∏–∫–µ. –í–æ–ª–Ω–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç 
    —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–≤–∞–Ω—Ç–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã. –ü—Ä–∏–Ω—Ü–∏–ø –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –ì–µ–π–∑–µ–Ω–±–µ—Ä–≥–∞ –±—ã–ª —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω 
    –≤ 1927 –≥–æ–¥—É. –ü—Ä–∏–Ω—Ü–∏–ø –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –ì–µ–π–∑–µ–Ω–±–µ—Ä–≥–∞ –≥–æ–≤–æ—Ä–∏—Ç –æ —Ç–æ–º, —á—Ç–æ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ 
    –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Ç–æ—á–Ω–æ –∏–∑–º–µ—Ä–∏—Ç—å –∏ –ø–æ–ª–æ–∂–µ–Ω–∏–µ, –∏ –∏–º–ø—É–ª—å—Å —á–∞—Å—Ç–∏—Ü—ã.
    """
    
    print("=== –¢–ï–°–¢ ML-–ú–û–î–£–õ–Ø ===")
    result = enhance_text(sample_text)
    print("\n–†–µ–∑—É–ª—å—Ç–∞—Ç:")
    print(result)
